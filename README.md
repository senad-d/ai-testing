# Testing AI capabilities

This repository is designed for assessing various AI models and their responses to different programming prompts. It includes a range of test cases aimed at measuring the effectiveness, accuracy, and reliability of AI-generated outputs. 
The goal is to provide a comprehensive suite of prompts that can be utilized for creating diverse projects. Additionally, the repository seeks to facilitate collaboration and knowledge sharing among developers and researchers working with AI models.

To accomplish this, use VSCode along with the "Continue" extension to run and evaluate the prompts effectively. This approach allows us to leverage multiple LLMs in the same environment, enhancing our understanding of their strengths and weaknesses.

## Table of Contents
1. Prerequisites
2. Getting Started
3. Documentation

## Prerequisites
- Visual Studio Code (VSCode)
- Continue extension for VSCode
- An AI model to test and compare

## Getting Started

1. Clone the repository:
    ```sh
    git clone https://github.com/yourusername/ai-testing.git
    cd ai-testing
    ```

2. Open the project in VSCode:
    ```sh
    code .
    ```

3. Ensure all necessary extensions and tools are installed and configured, including the Continue extension for VSCode to evaluate AI model prompts.

## Documentation
- [VSCode](https://code.visualstudio.com/)
- [Continue Extension for VSCode](https://docs.continue.dev/getting-started/overview)

We welcome contributions from the community to expand and improve the test prompts. Please feel free to submit pull requests with new prompts or enhancements to existing ones.